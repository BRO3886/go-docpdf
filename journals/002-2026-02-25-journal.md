# Journal Entry 002 - 2026-02-25 - Docker build, GHCR push, go.mod fix

## Goal

Build and push a production Docker image to GHCR (`ghcr.io/bro3886/go-docpdf`) tagged with both `:latest` and the short git SHA. Also investigated image size vs Gotenberg.

## What Changed

- `Dockerfile`: fixed base image tag — went through `golang:1.24.0-alpine` → `golang:1-alpine` (workaround) → back to `golang:1.24.0-alpine` once Docker Hub recovered
- `go.mod`: lowered minimum Go version from `1.24.5` → `1.24.0` to match the builder image
- Docker image built: `ghcr.io/bro3886/go-docpdf:latest` + `ghcr.io/bro3886/go-docpdf:bb80ed7` (917MB)
- Both tags pushed to GHCR after re-authing with `write:packages` scope
- GitHub repo created: `github.com/BRO3886/go-docpdf` (public), all commits pushed

## Key Insights

**`golang:1.24.0-alpine` vs `go.mod` version conflict:**
`go.mod` had `go 1.24.5` (stamped by local toolchain), but `golang:1.24.0-alpine` only has Go 1.24.0. With `GOTOOLCHAIN=local`, `go mod download` fails immediately:
```
go: go.mod requires go >= 1.24.5 (running go 1.24.0; GOTOOLCHAIN=local)
```
Fix: lower `go.mod` to `go 1.24.0` — no 1.24.5-specific features used. Alternatively could set `GOTOOLCHAIN=auto` in the Dockerfile but lowering go.mod is cleaner.

**Docker Hub throughput issue:**
`registry-1.docker.io` had severe slowness (~800ms TTFB, layer pulls timing out after minutes). `production.cloudflare.docker.com` (used for smaller/more popular images like `alpine`) was fine at 40ms. Root cause: peering issue between ISP and Docker Hub origin, not a Docker Hub outage (status page showed all green). Workaround: switched temporarily to `golang:1-alpine` (served via Cloudflare CDN edge), reverted once Hub recovered.

**OrbStack BuildKit drops connection on slow pulls:**
When layer pulls take too long, OrbStack's BuildKit daemon closes the gRPC stream with `EOF`. The build fails even though the pull is technically still in progress. Workaround: `docker pull` the slow image first to cache it, then `docker build` — BuildKit uses the cache and never needs to hit the registry.

**GHCR `write:packages` scope:**
`gh auth status` showed the token had `repo` but not `write:packages`. Docker login with that token succeeds (GHCR accepts it for auth) but push returns `permission_denied`. Fix: `gh auth refresh -s write:packages` then `docker login ghcr.io -u <user> --password-stdin <<< "$(gh auth token)"`.

**SHA tag mismatch:**
Built image with `rev-parse --short HEAD` at build time, but subsequent commits changed HEAD before the push command ran. The tag `bb80ed7` didn't exist locally because it was computed at a different point. Fix: always `docker tag latest <sha>` just before push rather than passing SHA at build time.

**Image size: 917MB** — entirely LibreOffice (251 Alpine packages). The Go binary is 5MB. For comparison: `gotenberg/gotenberg:8` is 1.86GB (adds Chromium + extras). `linuxserver/libreoffice` is 2.36GB (full desktop GUI). No useful slim LibreOffice base image exists on Docker Hub — `apk add libreoffice` on Alpine is already the minimal approach.

## Decisions Made

- **`go 1.24.0` in go.mod** — matches the pinned builder image, no features above 1.24.0 used, safe to lower
- **Pin builder to `golang:1.24.0-alpine`** (not floating `1-alpine`) — reproducible builds; the temporary `1-alpine` workaround was reverted once Hub recovered
- **Accept 917MB image size** — no viable path to meaningfully smaller without replacing LibreOffice, which would sacrifice PDF fidelity
- **SHA tag computed at push time via `docker tag`** — more reliable than passing SHA as a build arg

---

## Session 2 — Observability (X-Request-ID, JSON logging, Prometheus /metrics)

## Goal

Add three observability features to prepare for production scale: request tracing via `X-Request-ID`, structured JSON logging, and a Prometheus-compatible `/metrics` endpoint. Constraint: stdlib only, zero new external dependencies.

## What Changed

**New packages:**
- `internal/metrics/metrics.go` — `Registry` with five atomic fields (`convSuccess`, `convTimeout`, `convFailed`, `convInFlight`, histogram). `ServeHTTP` renders Prometheus text format (`text/plain; version=0.0.4`). Histogram stores cumulative per-bucket counts (each `observe(ms)` increments all buckets where `ms <= le`), rendered directly without re-accumulating.
- `internal/metrics/metrics_test.go` — 5 tests: counters, gauge, bucket placement, content-type, 50-goroutine race test.
- `internal/middleware/middleware.go` — Three middleware + context helpers:
  - `RequestID`: reuses `X-Request-ID` header or generates UUIDv4 via `crypto/rand`; echos on response
  - `Logging`: wraps a `responseRecorder` to capture status code, emits JSON to `os.Stderr` after each request; includes `"error"` field (omitempty) from `SetLogError`
  - `Metrics`: wraps `/convert` only; tracks in-flight gauge, duration histogram, and outcome counter
  - `SetOutcome` / `SetLogError` / `RequestIDFromContext`: nil-safe — no-op when no state on context, preserving all 15 existing tests
- `internal/middleware/middleware_test.go` + `middleware_test_helpers_test.go` — 9 tests including in-flight channel sync test and stderr capture/JSON parse test

**Modified files:**
- `internal/handler/handler.go`: added `SetOutcome` + `SetLogError` at every return site (23 middleware call sites total)
- `cmd/server/main.go`: wired `Registry`, added `/metrics` route, `RequestID→Logging→mux` chain, JSON startup log via `json.Marshal` + `fmt.Fprintf(os.Stderr, ...)`
- `README.md`: added "why this exists" paragraph
- `.claude/commands/release.md`: new `/release` skill for tag+GHCR push workflow

**Total tests: 19** (15 existing + 4 new packages — all pass with `-race`)

## Key Insights

**Histogram double-accumulation bug:**
Initial `writeTo` code had:
```go
var cumulative int64
for i, le := range histBuckets {
    cumulative += r.hist.counts[i].Load()  // BUG: re-accumulates already-cumulative counts
    fmt.Fprintf(w, "...{le=%d} %d\n", le, cumulative)
}
```
`observe` already stores cumulative counts (adds to all `le >= ms` buckets). `writeTo` was accumulating them again → bucket values grew by N*(N+1)/2 pattern. Fix: remove the accumulation in `writeTo`, render `counts[i]` directly.

**stderr capture race with `-race` detector:**
`captureStderr` spawned a goroutine draining pipe → `bytes.Buffer`. Test read `buf.String()` while the goroutine was still writing → data race. Fix: changed helper signature to return a `flush func() string` — the test calls `restoreStderr` first (which doesn't close the pipe), then calls `flush()` which closes the write-end and blocks until the goroutine drains. No mutex needed.

**Middleware state nil-safety is critical:**
All 15 existing handler tests use `httptest.NewRequest` without the `RequestID` middleware — no `*requestState` on context. `SetOutcome`/`SetLogError` must silently no-op. Pattern used:
```go
func SetOutcome(ctx context.Context, outcome string) {
    if s, ok := ctx.Value(contextKey{}).(*requestState); ok && s != nil {
        s.outcome = outcome
    }
}
```

**`responseRecorder.status` default:**
`WriteHeader` is not called for 200 responses in some handlers (they call `w.Write` directly which implicitly sends 200). `responseRecorder.status` initialised to `http.StatusOK` so the log line shows 200 not 0.

## Decisions Made

- **Histogram buckets are stored cumulatively** — matches Prometheus convention exactly; render is a direct load, no computation
- **`Metrics` middleware wraps only `/convert`** — health checks and `/metrics` scrapes must not pollute conversion counters or histogram
- **`Logging` wraps the entire mux** — single log line per request regardless of route, includes request ID set upstream by `RequestID`
- **JSON log to `os.Stderr`** — follows 12-factor; stderr is captured by container runtimes (Docker, k8s) and forwarded to log aggregators
- **`flush func()` pattern for stderr capture in tests** — cleaner than closing the pipe in `t.Cleanup` and synchronising with a mutex

---
